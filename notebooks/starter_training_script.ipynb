{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight Training Script with distilgpt2! \n",
    "\n",
    "This script aims to use transcripts the whisper-gpt team has collected, and trains a minimal gpt model on them.\n",
    "Specify dataset path, model desired, block size for training, and number of epochs below before running the script.\n",
    "We tokenize input data, block them to allow for better processing, and pass them to our model for training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting starter_model_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile starter_model_training.py\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, create_optimizer, AdamWeightDecay, TFAutoModelForCausalLM\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import pipeline\n",
    "import time\n",
    "#most code is taken from the old huggingface script for language modeling with tensorflow\n",
    "\n",
    "\n",
    "def tokenize_function(dat, model_checkpoint):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    return tokenizer(dat[\"text\"])\n",
    "\n",
    "def group_texts(dat, block_size = 64):\n",
    "    # function taken directly from HF script used to chunk data into block_size\n",
    "    # Concatenate all texts\n",
    "\n",
    "    concatenated_examples = {k: sum(dat[k], []) for k in dat.keys()}\n",
    "    total_length = len(concatenated_examples[list(dat.keys())[0]])\n",
    "\n",
    "    # We drop the small remainder, though you could add padding instead if the model supports it\n",
    "    # In this, as in all things, we advise you to follow your heart\n",
    "    \n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    \n",
    "    # Split by chunks of max_len.\n",
    "    \n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "def compile_model(model_checkpoint, lr = 2e-5, weight_decay_rate = 0.01):\n",
    "    # Retrieve a model from model_checkpoint, and load with optimizer\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "    optimizer = AdamWeightDecay(lr=lr, weight_decay_rate=weight_decay_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_dataset(dataset_path, model_checkpoint):\n",
    "    # tokenize, batch, prepare for model dev\n",
    "    datasets = load_dataset(dataset_path)\n",
    "\n",
    "    tokenized_datasets = datasets.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        num_proc=4, \n",
    "        remove_columns = [\"text\", \"id\", \"segments\"],\n",
    "        fn_kwargs={\"model_checkpoint\": model_checkpoint}\n",
    "    )\n",
    "    # chunk the data\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        group_texts,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=4,\n",
    "    )\n",
    "\n",
    "    return lm_datasets\n",
    "\n",
    "\n",
    "def gen_text(model_checkpoint, model, seed_text, num_return_sequences = 3):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    p = pipeline(\"text-generation\", model = model, tokenizer = tokenizer)\n",
    "    genned_text = p(seed_text, num_return_sequences = num_return_sequences)\n",
    "    genned_text = [x[\"generated_text\"] for x in genned_text]\n",
    "    return \" \".join(genned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-17 19:36:38.874701: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/ArjunPatel/opt/anaconda3/envs/whisper-gpt/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from starter_model_training import *\n",
    "\n",
    "DATASET_PATH = \"Whispering-GPT/whisper-transcripts-the-verge\"\n",
    "MODEL_CHECKPOINT = \"distilgpt2\"\n",
    "BLOCK_SIZE = 64\n",
    "EPOCHS = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Whispering-GPT--whisper-transcripts-the-verge-423edd370c197473\n",
      "Found cached dataset json (/Users/ArjunPatel/.cache/huggingface/datasets/Whispering-GPT___json/Whispering-GPT--whisper-transcripts-the-verge-423edd370c197473/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n",
      "100%|██████████| 1/1 [00:00<00:00, 179.85it/s]\n",
      "Loading cached processed dataset at /Users/ArjunPatel/.cache/huggingface/datasets/Whispering-GPT___json/Whispering-GPT--whisper-transcripts-the-verge-423edd370c197473/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-3266fba3f757ad7c.arrow\n",
      "Loading cached processed dataset at /Users/ArjunPatel/.cache/huggingface/datasets/Whispering-GPT___json/Whispering-GPT--whisper-transcripts-the-verge-423edd370c197473/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-ac382d491e73500d.arrow\n",
      "Loading cached processed dataset at /Users/ArjunPatel/.cache/huggingface/datasets/Whispering-GPT___json/Whispering-GPT--whisper-transcripts-the-verge-423edd370c197473/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-a954f1605656c0a8.arrow\n",
      "Loading cached processed dataset at /Users/ArjunPatel/.cache/huggingface/datasets/Whispering-GPT___json/Whispering-GPT--whisper-transcripts-the-verge-423edd370c197473/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-5b7032ecbc4dbba7.arrow\n",
      "Loading cached processed dataset at /Users/ArjunPatel/.cache/huggingface/datasets/Whispering-GPT___json/Whispering-GPT--whisper-transcripts-the-verge-423edd370c197473/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-e0daca45618b8468.arrow\n",
      "Loading cached processed dataset at /Users/ArjunPatel/.cache/huggingface/datasets/Whispering-GPT___json/Whispering-GPT--whisper-transcripts-the-verge-423edd370c197473/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-4eefd3a5c1c08759.arrow\n",
      "Loading cached processed dataset at /Users/ArjunPatel/.cache/huggingface/datasets/Whispering-GPT___json/Whispering-GPT--whisper-transcripts-the-verge-423edd370c197473/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-444c580ed6f006d9.arrow\n",
      "Loading cached processed dataset at /Users/ArjunPatel/.cache/huggingface/datasets/Whispering-GPT___json/Whispering-GPT--whisper-transcripts-the-verge-423edd370c197473/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab/cache-ea1ecff457747481.arrow\n"
     ]
    }
   ],
   "source": [
    "d = create_dataset(DATASET_PATH, model_checkpoint=MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "/Users/ArjunPatel/opt/anaconda3/envs/whisper-gpt/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "model = compile_model(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "train_set = d[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65/65 [==============================] - 168s 2s/step - loss: 3.7218\n"
     ]
    }
   ],
   "source": [
    "mod_history = model.fit(train_set, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as wte_layer_call_fn, wte_layer_call_and_return_conditional_losses, dropout_76_layer_call_fn, dropout_76_layer_call_and_return_conditional_losses, ln_f_layer_call_fn while saving (showing 5 of 150). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "/Users/ArjunPatel/opt/anaconda3/envs/whisper-gpt/lib/python3.10/site-packages/transformers/generation_tf_utils.py:1690: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Transcript of the newest The Verge YouTube video about the latest new cell phone: \\xa0 It features four new features, a four-megapixel camera and a built-in camera, as well as 2.0\" of HD video recording and 5'},\n",
       " {'generated_text': 'Transcript of the newest The Verge YouTube video about the latest new cell phone: -------------------------- T. P. Harnik, who is the president of the New York Public Radio Alliance, has a different perspective on how technology is being developed,'},\n",
       " {'generated_text': \"Transcript of the newest The Verge YouTube video about the latest new cell phone: _____________. _____________. The Verge's new video about the latest new cell phone: _____________. The Verge's new video about the newest cell phone\"}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def gen_text(model_checkpoint, model, seed_text, num_return_sequences = 3):\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "#     p = pipeline(\"text-generation\", model = model, tokenizer = tokenizer)\n",
    "#     return p(seed_text, num_return_sequences = num_return_sequences)\n",
    "\n",
    "gen_text(MODEL_CHECKPOINT, model, \"Transcript of the newest The Verge YouTube video about the latest new cell phone: \", 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('whisper-gpt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99d3b09c14b8827529092680aa0731e618985fe35070613d2f14bbe882ee2faa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
