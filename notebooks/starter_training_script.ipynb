{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, create_optimizer, AdamWeightDecay, TFAutoModelForCausalLM\n",
    "from transformers import DefaultDataCollator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightweight Training Script with distilgpt2! \n",
    "\n",
    "This script aims to use transcripts the whisper-gpt team has collected, and trains a minimal gpt model on them.\n",
    "Specify dataset path, model desired, block size for training, and number of epochs below before running the script.\n",
    "We tokenize input data, block them to allow for better processing, and pass them to our model for training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most code is taken from the old huggingface script for language modeling with \n",
    "DATASET_PATH = \"kpriyanshu256/whisper-transcripts\"\n",
    "MODEL_CHECKPOINT = \"distilgpt2\"\n",
    "BLOCK_SIZE = 64\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(dat, model_checkpoint = MODEL_CHECKPOINT):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    return tokenizer(dat[\"text\"])\n",
    "\n",
    "def group_texts(dat, block_size = BLOCK_SIZE):\n",
    "    # function from HF script used to chunk data into block_size\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(dat[k], []) for k in dat.keys()}\n",
    "    total_length = len(concatenated_examples[list(dat.keys())[0]])\n",
    "    # We drop the small remainder, though you could add padding instead if the model supports it\n",
    "    # In this, as in all things, we advise you to follow your heart\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + BLOCK_SIZE] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "def compile_model(model_checkpoint = MODEL_CHECKPOINT, lr = 2e-5, weight_decay_rate = 0.01):\n",
    "    # Retrieve a model from model_checkpoint, and load with optimizer\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "    optimizer = AdamWeightDecay(lr=lr, weight_decay_rate=weight_decay_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration kpriyanshu256--whisper-transcripts-b310a43c8142e04a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/kpriyanshu256--whisper-transcripts to /Users/ArjunPatel/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-b310a43c8142e04a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 64.5M/64.5M [00:01<00:00, 35.9MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:03<00:00,  3.26s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 646.37it/s]\n",
      "                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/ArjunPatel/.cache/huggingface/datasets/kpriyanshu256___json/kpriyanshu256--whisper-transcripts-b310a43c8142e04a/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 104.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize and preprocess dataset\n",
    "datasets = load_dataset(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[AToken indices sequence length is longer than the specified maximum sequence length for this model (2799 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2086 > 1024). Running this sequence through the model will result in indexing errors\n",
      "#0: 100%|██████████| 1/1 [00:01<00:00,  1.67s/ba]\n",
      "\n",
      "\n",
      "#3: 100%|██████████| 1/1 [00:01<00:00,  1.64s/ba]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1202 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 1024). Running this sequence through the model will result in indexing errors\n",
      "#1: 100%|██████████| 1/1 [00:01<00:00,  1.82s/ba]\n",
      "\n",
      "#2: 100%|██████████| 1/1 [00:01<00:00,  1.85s/ba]\n",
      "#0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "#0: 100%|██████████| 1/1 [00:00<00:00, 16.20ba/s]\n",
      "\n",
      "\n",
      "#1: 100%|██████████| 1/1 [00:00<00:00, 17.35ba/s]\n",
      "#2: 100%|██████████| 1/1 [00:00<00:00, 26.97ba/s]\n",
      "#3: 100%|██████████| 1/1 [00:00<00:00, 15.09ba/s]\n",
      "2022-10-20 20:30:27.747215: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "/Users/ArjunPatel/opt/anaconda3/envs/whisper-gpt/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "# apply tokenization\n",
    "tokenized_datasets = datasets.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    num_proc=4, \n",
    "    remove_columns = [\"text\", \"id\", \"segments\"])\n",
    "\n",
    "# chunk the data\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "model = compile_model(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "train_set = lm_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 175s 3s/step - loss: 3.7119\n"
     ]
    }
   ],
   "source": [
    "mod_history = model.fit(train_set, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as wte_layer_call_fn, wte_layer_call_and_return_conditional_losses, dropout_19_layer_call_fn, dropout_19_layer_call_and_return_conditional_losses, ln_f_layer_call_fn while saving (showing 5 of 150). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"trained_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('whisper-gpt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "99d3b09c14b8827529092680aa0731e618985fe35070613d2f14bbe882ee2faa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
